{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbrva3PDOnep",
        "outputId": "0dafad67-e298-4f39-ae4a-8eb11349d93f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input X shape: torch.Size([1, 3, 4])\n",
            "\n",
            "Query (Q):\n",
            " tensor([[[2.5704, 2.1938, 1.9302, 2.1321],\n",
            "         [2.7281, 1.3986, 2.1553, 2.0644],\n",
            "         [2.5911, 1.9027, 2.1888, 2.6103]]])\n",
            "\n",
            "Key (K):\n",
            " tensor([[[1.8067, 0.7406, 2.3333, 1.2959],\n",
            "         [1.3170, 0.9812, 1.2806, 0.9641],\n",
            "         [1.1918, 1.1584, 1.9042, 1.9380]]])\n",
            "\n",
            "Value (V):\n",
            " tensor([[[2.5990, 1.7584, 2.3007, 1.9257],\n",
            "         [2.2320, 1.2705, 1.7976, 2.5233],\n",
            "         [2.5792, 2.4538, 2.6084, 2.2205]]])\n",
            "\n",
            "Attention Scores:\n",
            " tensor([[[6.7678, 5.0326, 6.7061],\n",
            "         [6.8346, 4.8579, 6.4882],\n",
            "         [7.2903, 5.2995, 7.2594]]])\n",
            "\n",
            "Attention Weights:\n",
            " tensor([[[0.4725, 0.0833, 0.4442],\n",
            "         [0.5418, 0.0750, 0.3832],\n",
            "         [0.4748, 0.0649, 0.4603]]])\n",
            "\n",
            "Context Vector (Final Output):\n",
            " tensor([[[2.5596, 2.0266, 2.3955, 2.1065],\n",
            "         [2.5639, 1.9882, 2.3809, 2.0835],\n",
            "         [2.5661, 2.0469, 2.4097, 2.1002]]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "# Set seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# New Input: batch_size=1, seq_len=3, embedding_dim=4\n",
        "X = torch.tensor(\n",
        "    [\n",
        "        [\n",
        "            [0.5, 1.0, 0.0, 2.0],\n",
        "            [1.0, 0.0, 1.5, 0.5],\n",
        "            [0.0, 2.0, 1.0, 1.0],\n",
        "        ]\n",
        "    ]\n",
        ")\n",
        "# Initialize weight matrices\n",
        "d_model = X.size(-1)\n",
        "W_Q = torch.rand(d_model, d_model)\n",
        "W_K = torch.rand(d_model, d_model)\n",
        "W_V = torch.rand(d_model, d_model)\n",
        "\n",
        "# Compute Query, Key, Value\n",
        "Q = torch.matmul(X, W_Q)\n",
        "K = torch.matmul(X, W_K)\n",
        "V = torch.matmul(X, W_V)\n",
        "\n",
        "\n",
        "# Scaled Dot-Product Attention\n",
        "scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_model)\n",
        "attention_weights = F.softmax(scores, dim=-1)\n",
        "context_vector = torch.matmul(attention_weights, V)\n",
        "\n",
        "# Outputs\n",
        "print(\"Input X shape:\", X.shape)\n",
        "print(\"\\nQuery (Q):\\n\", Q)\n",
        "print(\"\\nKey (K):\\n\", K)\n",
        "print(\"\\nValue (V):\\n\", V)\n",
        "print(\"\\nAttention Scores:\\n\", scores)\n",
        "print(\"\\nAttention Weights:\\n\", attention_weights)\n",
        "print(\"\\nContext Vector (Final Output):\\n\", context_vector)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VfKhpfd8Op7w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}